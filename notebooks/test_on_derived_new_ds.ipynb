{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv(\"/Users/calebjonesshibu/Desktop/tom/derived/draft_2023_06_05_11/nirs/exp_2022_09_30_10/leopard_affective_individual_physio_task.csv\")\n",
    "data_2 = pd.read_csv(\"/Users/calebjonesshibu/Desktop/tom/derived/draft_2023_06_05_11/nirs/exp_2022_09_30_10/lion_affective_individual_physio_task.csv\")\n",
    "data_3 = pd.read_csv(\"/Users/calebjonesshibu/Desktop/tom/derived/draft_2023_06_05_11/nirs/exp_2022_09_30_10/tiger_affective_individual_physio_task.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.drop(columns=['unix_time', 'task_time', 'task_monotonic_time', 'task_human_readable_time', 'task_subject_id', 'seconds_since_start', 'human_readable_time', 'leopard', 'task_index', 'experiment_id'], inplace=True)\n",
    "data_2.drop(columns=['unix_time', 'task_time', 'task_monotonic_time', 'task_human_readable_time', 'task_subject_id', 'seconds_since_start', 'human_readable_time', 'lion', 'task_index', 'experiment_id'], inplace=True)\n",
    "data_3.drop(columns=['unix_time', 'task_time', 'task_monotonic_time', 'task_human_readable_time', 'task_subject_id', 'seconds_since_start', 'human_readable_time', 'tiger', 'task_index', 'experiment_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill NaNs to 'task_arousal_score', 'task_valence_score' with task_event_type == 'intermediate_selection'\n",
    "data_1.loc[data_1['task_event_type'] == 'intermediate_selection', ['task_arousal_score', 'task_valence_score']] = np.nan\n",
    "data_2.loc[data_2['task_event_type'] == 'intermediate_selection', ['task_arousal_score', 'task_valence_score']] = np.nan\n",
    "data_3.loc[data_3['task_event_type'] == 'intermediate_selection', ['task_arousal_score', 'task_valence_score']] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backfill 'task_arousal_score', 'task_valence_score' when task_event_type == 'final_submission'\n",
    "data_1[['task_image_path', 'task_arousal_score', 'task_valence_score', 'task_event_type']] = data_1[['task_image_path', 'task_arousal_score', 'task_valence_score', 'task_event_type']].fillna(method='bfill')\n",
    "data_2[['task_image_path', 'task_arousal_score', 'task_valence_score', 'task_event_type']] = data_2[['task_image_path', 'task_arousal_score', 'task_valence_score', 'task_event_type']].fillna(method='bfill')\n",
    "data_3[['task_image_path', 'task_arousal_score', 'task_valence_score', 'task_event_type']] = data_3[['task_image_path', 'task_arousal_score', 'task_valence_score', 'task_event_type']].fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.dropna(inplace=True)\n",
    "data_2.dropna(inplace=True)\n",
    "data_3.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.drop(columns=['task_image_path', 'task_event_type'], inplace=True)\n",
    "data_2.drop(columns=['task_image_path', 'task_event_type'], inplace=True)\n",
    "data_3.drop(columns=['task_image_path', 'task_event_type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the column names to None for each dataframe\n",
    "data_1.columns = [None] * len(data_1.columns)\n",
    "data_2.columns = [None] * len(data_2.columns)\n",
    "data_3.columns = [None] * len(data_3.columns)\n",
    "\n",
    "# Concatenating the dataframes vertically\n",
    "merged_df = pd.concat([data_1, data_2, data_3], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\n",
    "    \"S1-D1_HbO\", \"S1-D2_HbO\", \"S2-D1_HbO\", \"S2-D3_HbO\", \"S3-D1_HbO\",\n",
    "    \"S3-D3_HbO\", \"S3-D4_HbO\", \"S4-D2_HbO\", \"S4-D4_HbO\", \"S4-D5_HbO\",\n",
    "    \"S5-D3_HbO\", \"S5-D4_HbO\", \"S5-D6_HbO\", \"S6-D4_HbO\", \"S6-D6_HbO\",\n",
    "    \"S6-D7_HbO\", \"S7-D5_HbO\", \"S7-D7_HbO\", \"S8-D6_HbO\", \"S8-D7_HbO\",\n",
    "    \"S1-D1_HbR\", \"S1-D2_HbR\", \"S2-D1_HbR\", \"S2-D3_HbR\", \"S3-D1_HbR\",\n",
    "    \"S3-D3_HbR\", \"S3-D4_HbR\", \"S4-D2_HbR\", \"S4-D4_HbR\", \"S4-D5_HbR\",\n",
    "    \"S5-D3_HbR\", \"S5-D4_HbR\", \"S5-D6_HbR\", \"S6-D4_HbR\", \"S6-D6_HbR\",\n",
    "    \"S6-D7_HbR\", \"S7-D5_HbR\", \"S7-D7_HbR\", \"S8-D6_HbR\", \"S8-D7_HbR\", \n",
    "    \"arousal_score\", \"valence_score\"\n",
    "]\n",
    "merged_df = merged_df.set_axis(headers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, Loss: 1.4146275520324707\n",
      "Epoch 20/1000, Loss: 1.753422737121582\n",
      "Epoch 30/1000, Loss: 1.2336524724960327\n",
      "Epoch 40/1000, Loss: 1.2750574350357056\n",
      "Epoch 50/1000, Loss: 1.3150863647460938\n",
      "Epoch 60/1000, Loss: 1.2093348503112793\n",
      "Epoch 70/1000, Loss: 1.2408185005187988\n",
      "Epoch 80/1000, Loss: 1.0053539276123047\n",
      "Epoch 90/1000, Loss: 1.3651790618896484\n",
      "Epoch 100/1000, Loss: 1.334029197692871\n",
      "Epoch 110/1000, Loss: 1.109188437461853\n",
      "Epoch 120/1000, Loss: 1.0301941633224487\n",
      "Epoch 130/1000, Loss: 1.0004347562789917\n",
      "Epoch 140/1000, Loss: 1.1032227277755737\n",
      "Epoch 150/1000, Loss: 1.2053205966949463\n",
      "Epoch 160/1000, Loss: 1.1237409114837646\n",
      "Epoch 170/1000, Loss: 1.1960358619689941\n",
      "Epoch 180/1000, Loss: 0.9522440433502197\n",
      "Epoch 190/1000, Loss: 0.9956511855125427\n",
      "Epoch 200/1000, Loss: 0.9995516538619995\n",
      "Epoch 210/1000, Loss: 0.9424712657928467\n",
      "Epoch 220/1000, Loss: 1.1944994926452637\n",
      "Epoch 230/1000, Loss: 1.3012149333953857\n",
      "Epoch 240/1000, Loss: 1.0925474166870117\n",
      "Epoch 250/1000, Loss: 1.3908073902130127\n",
      "Epoch 260/1000, Loss: 1.1941673755645752\n",
      "Epoch 270/1000, Loss: 1.2931132316589355\n",
      "Epoch 280/1000, Loss: 1.1238985061645508\n",
      "Epoch 290/1000, Loss: 1.1956257820129395\n",
      "Epoch 300/1000, Loss: 1.2947239875793457\n",
      "Epoch 310/1000, Loss: 1.2974557876586914\n",
      "Epoch 320/1000, Loss: 0.8280543088912964\n",
      "Epoch 330/1000, Loss: 0.8952500224113464\n",
      "Epoch 340/1000, Loss: 1.2911769151687622\n",
      "Epoch 350/1000, Loss: 0.9967169165611267\n",
      "Epoch 360/1000, Loss: 1.1986722946166992\n",
      "Epoch 370/1000, Loss: 0.9953749775886536\n",
      "Epoch 380/1000, Loss: 0.9960973262786865\n",
      "Epoch 390/1000, Loss: 0.7011022567749023\n",
      "Epoch 400/1000, Loss: 0.9954483509063721\n",
      "Epoch 410/1000, Loss: 1.3965195417404175\n",
      "Epoch 420/1000, Loss: 1.0290255546569824\n",
      "Epoch 430/1000, Loss: 0.9979077577590942\n",
      "Epoch 440/1000, Loss: 1.2942979335784912\n",
      "Epoch 450/1000, Loss: 1.0325756072998047\n",
      "Epoch 460/1000, Loss: 1.190065622329712\n",
      "Epoch 470/1000, Loss: 1.1903955936431885\n",
      "Epoch 480/1000, Loss: 1.1928751468658447\n",
      "Epoch 490/1000, Loss: 1.0701425075531006\n",
      "Epoch 500/1000, Loss: 1.1389415264129639\n",
      "Epoch 510/1000, Loss: 1.219250202178955\n",
      "Epoch 520/1000, Loss: 1.2925493717193604\n",
      "Epoch 530/1000, Loss: 1.292781114578247\n",
      "Epoch 540/1000, Loss: 1.0954374074935913\n",
      "Epoch 550/1000, Loss: 1.1700072288513184\n",
      "Epoch 560/1000, Loss: 1.1780632734298706\n",
      "Epoch 570/1000, Loss: 1.0938348770141602\n",
      "Epoch 580/1000, Loss: 1.091935157775879\n",
      "Epoch 590/1000, Loss: 1.194441556930542\n",
      "Epoch 600/1000, Loss: 0.9936815500259399\n",
      "Epoch 610/1000, Loss: 0.8993180990219116\n",
      "Epoch 620/1000, Loss: 1.2113921642303467\n",
      "Epoch 630/1000, Loss: 0.9919865131378174\n",
      "Epoch 640/1000, Loss: 1.190425157546997\n",
      "Epoch 650/1000, Loss: 1.1904833316802979\n",
      "Epoch 660/1000, Loss: 1.2177425622940063\n",
      "Epoch 670/1000, Loss: 1.1910252571105957\n",
      "Epoch 680/1000, Loss: 1.0072625875473022\n",
      "Epoch 690/1000, Loss: 1.2187639474868774\n",
      "Epoch 700/1000, Loss: 1.2024484872817993\n",
      "Epoch 710/1000, Loss: 1.1899497509002686\n",
      "Epoch 720/1000, Loss: 1.0915768146514893\n",
      "Epoch 730/1000, Loss: 0.7927799224853516\n",
      "Epoch 740/1000, Loss: 0.9928386211395264\n",
      "Epoch 750/1000, Loss: 1.0989224910736084\n",
      "Epoch 760/1000, Loss: 0.7929340600967407\n",
      "Epoch 770/1000, Loss: 1.1149307489395142\n",
      "Epoch 780/1000, Loss: 1.020080804824829\n",
      "Epoch 790/1000, Loss: 1.194516658782959\n",
      "Epoch 800/1000, Loss: 1.2075464725494385\n",
      "Epoch 810/1000, Loss: 0.9915566444396973\n",
      "Epoch 820/1000, Loss: 1.0928304195404053\n",
      "Epoch 830/1000, Loss: 0.8946242332458496\n",
      "Epoch 840/1000, Loss: 0.9289982318878174\n",
      "Epoch 850/1000, Loss: 1.0927019119262695\n",
      "Epoch 860/1000, Loss: 1.0908094644546509\n",
      "Epoch 870/1000, Loss: 1.3091249465942383\n",
      "Epoch 880/1000, Loss: 0.8946666121482849\n",
      "Epoch 890/1000, Loss: 1.2815535068511963\n",
      "Epoch 900/1000, Loss: 1.0918376445770264\n",
      "Epoch 910/1000, Loss: 1.0907191038131714\n",
      "Epoch 920/1000, Loss: 1.0917370319366455\n",
      "Epoch 930/1000, Loss: 1.2949097156524658\n",
      "Epoch 940/1000, Loss: 0.8271443843841553\n",
      "Epoch 950/1000, Loss: 1.0921578407287598\n",
      "Epoch 960/1000, Loss: 1.0901726484298706\n",
      "Epoch 970/1000, Loss: 1.190317153930664\n",
      "Epoch 980/1000, Loss: 1.0906267166137695\n",
      "Epoch 990/1000, Loss: 1.0926272869110107\n",
      "Epoch 1000/1000, Loss: 0.8924778699874878\n",
      "Accuracy of the network on the test data for arousal_score: 55.62381852551985 %\n",
      "Accuracy of the network on the test data for valence_score: 64.55576559546314 %\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocess data\n",
    "features = merged_df.iloc[:, :-2].values\n",
    "arousal_score = LabelEncoder().fit_transform(merged_df.iloc[:, -2] + 2)  # Mapping -2 -> 0, -1 -> 1, 0 -> 2, 1 -> 3, 2 -> 4\n",
    "valence_score = LabelEncoder().fit_transform(merged_df.iloc[:, -1] + 2)  # Same mapping for valence_score\n",
    "targets = list(zip(arousal_score, valence_score))\n",
    "\n",
    "# Split data\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = features.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = 5  # Classes representing -2, -1, 0, 1, 2\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(torch.tensor(train_features).float().to(device), torch.tensor(train_targets).long().to(device))\n",
    "test_data = TensorDataset(torch.tensor(test_features).float().to(device), torch.tensor(test_targets).long().to(device))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LSTM(input_size, hidden_size, num_classes).to(device)  # Move the model to the GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1, 1, input_size)\n",
    "        targets_arousal = targets[:, 0]\n",
    "        targets_valence = targets[:, 1]\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss_arousal = criterion(outputs, targets_arousal)\n",
    "        loss_valence = criterion(outputs, targets_valence)\n",
    "\n",
    "        loss = loss_arousal + loss_valence  # Total loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Testing\n",
    "correct_arousal, correct_valence = 0, 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.view(-1, 1, input_size)\n",
    "        targets_arousal = targets[:, 0]\n",
    "        targets_valence = targets[:, 1]\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted_arousal = torch.max(outputs.data, 1)\n",
    "        _, predicted_valence = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct_arousal += (predicted_arousal == targets_arousal).sum().item()\n",
    "        correct_valence += (predicted_valence == targets_valence).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test data for arousal_score: {100 * correct_arousal / total} %')\n",
    "print(f'Accuracy of the network on the test data for valence_score: {100 * correct_valence / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 10/10, Loss: 2.1788594722747803\n",
      "Fold 2/5\n",
      "Epoch 10/10, Loss: 1.5521869659423828\n",
      "Fold 3/5\n",
      "Epoch 10/10, Loss: 1.0089495182037354\n",
      "Fold 4/5\n",
      "Epoch 10/10, Loss: 1.2936415672302246\n",
      "Fold 5/5\n",
      "Epoch 10/10, Loss: 1.2802364826202393\n",
      "Average accuracy for arousal_score: 55.75777929721541\n",
      "Standard deviation for arousal_score: 4.0242982639874985\n",
      "Average accuracy for valence_score: 61.93970513972123\n",
      "Standard deviation for valence_score: 2.260024175119987\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m# Plot train and test loss\u001b[39;00m\n\u001b[1;32m    116\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m--> 117\u001b[0m plt\u001b[39m.\u001b[39;49mplot(\u001b[39mrange\u001b[39;49m(\u001b[39m1\u001b[39;49m, num_epochs \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), fold_losses, label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTrain Loss\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    118\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mEpochs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/ToMCAT-ML-Modeling/.conda/lib/python3.11/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/ToMCAT-ML-Modeling/.conda/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/Desktop/ToMCAT-ML-Modeling/.conda/lib/python3.11/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/Desktop/ToMCAT-ML-Modeling/.conda/lib/python3.11/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocess data\n",
    "features = merged_df.iloc[:, :-2].values\n",
    "arousal_score = LabelEncoder().fit_transform(merged_df.iloc[:, -2] + 2)  # Mapping -2 -> 0, -1 -> 1, 0 -> 2, 1 -> 3, 2 -> 4\n",
    "valence_score = LabelEncoder().fit_transform(merged_df.iloc[:, -1] + 2)  # Same mapping for valence_score\n",
    "targets = list(zip(arousal_score, valence_score))\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = features.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = 5  # Classes representing -2, -1, 0, 1, 2\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_folds = 5\n",
    "\n",
    "# Create DataLoaders\n",
    "dataset = TensorDataset(torch.tensor(features).float().to(device), torch.tensor(targets).long().to(device))\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Define model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = LSTM(input_size, hidden_size, num_classes).to(device)  # Move the model to the GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "fold_losses = []\n",
    "fold_accuracies = []\n",
    "\n",
    "for fold, (train_indices, test_indices) in enumerate(kfold.split(dataset)):\n",
    "    print(f\"Fold {fold+1}/{num_folds}\")\n",
    "\n",
    "    # Split data into train and test sets for the current fold\n",
    "    train_data = Subset(dataset, train_indices)\n",
    "    test_data = Subset(dataset, test_indices)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs = inputs.view(-1, 1, input_size)\n",
    "            targets_arousal = targets[:, 0]\n",
    "            targets_valence = targets[:, 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss_arousal = criterion(outputs, targets_arousal)\n",
    "            loss_valence = criterion(outputs, targets_valence)\n",
    "\n",
    "            loss = loss_arousal + loss_valence  # Total loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    fold_losses.append(loss.item())\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    correct_arousal, correct_valence = 0, 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.view(-1, 1, input_size)\n",
    "            targets_arousal = targets[:, 0]\n",
    "            targets_valence = targets[:, 1]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, predicted_arousal = torch.max(outputs.data, 1)\n",
    "            _, predicted_valence = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += targets.size(0)\n",
    "            correct_arousal += (predicted_arousal == targets_arousal).sum().item()\n",
    "            correct_valence += (predicted_valence == targets_valence).sum().item()\n",
    "\n",
    "    accuracy_arousal = 100 * correct_arousal / total\n",
    "    accuracy_valence = 100 * correct_valence / total\n",
    "    fold_accuracies.append((accuracy_arousal, accuracy_valence))\n",
    "\n",
    "# Print average accuracy and standard deviation across folds\n",
    "arousal_accuracies, valence_accuracies = zip(*fold_accuracies)\n",
    "print(\"Average accuracy for arousal_score:\", np.mean(arousal_accuracies))\n",
    "print(\"Standard deviation for arousal_score:\", np.std(arousal_accuracies))\n",
    "print(\"Average accuracy for valence_score:\", np.mean(valence_accuracies))\n",
    "print(\"Standard deviation for valence_score:\", np.std(valence_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
