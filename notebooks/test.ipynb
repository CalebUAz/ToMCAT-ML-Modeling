{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_affective = pd.read_csv(\"/Users/calebjonesshibu/Desktop/tom/data/exp_2023_02_03_10/baseline_tasks/affective/individual_00104_1675449225.csv\", sep=\";\")\n",
    "# extracted_affective = pd.read_csv(\"/Users/calebjonesshibu/Desktop/Neurips/Affective_Task_Individual/exp_2023_02_03_10/104.csv\", sep=\",\")\n",
    "# # extracted_affective.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # raw_affective['arousal_score'].fillna(method='ffill', inplace=True)\n",
    "# # raw_affective['valence_score'].fillna(method='ffill', inplace=True)\n",
    "# # raw_affective['subject_id'].fillna(method='ffill', inplace=True)\n",
    "# # Forward fill NaN values in all columns\n",
    "# raw_affective.fillna(method='bfill', inplace=True)\n",
    "# extracted_affective.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_affective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_affective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming you have the DataFrames raw_affective and extracted_affective\n",
    "\n",
    "# # Convert 'human_readable_time' column in extracted_affective to datetime type\n",
    "# extracted_affective['human_readable_time'] = pd.to_datetime(extracted_affective['human_readable_time'])\n",
    "\n",
    "# # Perform the merge based on closest matching time\n",
    "# merged_df = pd.merge_asof(extracted_affective.sort_values('unix_time'),\n",
    "#                           raw_affective.sort_values('time'),\n",
    "#                           left_on='unix_time',\n",
    "#                           right_on='time',\n",
    "#                           direction='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('/Users/calebjonesshibu/Desktop/Neurips/Affective_Task_Individual_rating/exp_2023_02_21_14/109.csv')\n",
    "data_1.drop(['unix_time', 'event_type_x', 'event_type_y', 'unix_time' ,'time', 'subject_id', 'image_path','human_readable_time_y', 'human_readable_time_x', 'monotonic_time'] , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1-D1_HbO</th>\n",
       "      <th>S1-D2_HbO</th>\n",
       "      <th>S2-D1_HbO</th>\n",
       "      <th>S2-D3_HbO</th>\n",
       "      <th>S3-D1_HbO</th>\n",
       "      <th>S3-D3_HbO</th>\n",
       "      <th>S3-D4_HbO</th>\n",
       "      <th>S4-D2_HbO</th>\n",
       "      <th>S4-D4_HbO</th>\n",
       "      <th>S4-D5_HbO</th>\n",
       "      <th>...</th>\n",
       "      <th>S5-D6_HbR</th>\n",
       "      <th>S6-D4_HbR</th>\n",
       "      <th>S6-D6_HbR</th>\n",
       "      <th>S6-D7_HbR</th>\n",
       "      <th>S7-D5_HbR</th>\n",
       "      <th>S7-D7_HbR</th>\n",
       "      <th>S8-D6_HbR</th>\n",
       "      <th>S8-D7_HbR</th>\n",
       "      <th>arousal_score</th>\n",
       "      <th>valence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.471891</td>\n",
       "      <td>0.202912</td>\n",
       "      <td>-0.093997</td>\n",
       "      <td>0.134281</td>\n",
       "      <td>0.372591</td>\n",
       "      <td>0.322716</td>\n",
       "      <td>0.232153</td>\n",
       "      <td>-0.256723</td>\n",
       "      <td>0.219447</td>\n",
       "      <td>0.666889</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171817</td>\n",
       "      <td>-0.041364</td>\n",
       "      <td>-0.126358</td>\n",
       "      <td>-0.169818</td>\n",
       "      <td>0.146536</td>\n",
       "      <td>-0.133517</td>\n",
       "      <td>-0.148632</td>\n",
       "      <td>-0.006896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.469043</td>\n",
       "      <td>0.198057</td>\n",
       "      <td>-0.108167</td>\n",
       "      <td>0.129077</td>\n",
       "      <td>0.367336</td>\n",
       "      <td>0.313904</td>\n",
       "      <td>0.238565</td>\n",
       "      <td>-0.258443</td>\n",
       "      <td>0.213415</td>\n",
       "      <td>0.635690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.171419</td>\n",
       "      <td>-0.040187</td>\n",
       "      <td>-0.127127</td>\n",
       "      <td>-0.166950</td>\n",
       "      <td>0.144688</td>\n",
       "      <td>-0.128004</td>\n",
       "      <td>-0.146233</td>\n",
       "      <td>-0.008951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.465940</td>\n",
       "      <td>0.193808</td>\n",
       "      <td>-0.122093</td>\n",
       "      <td>0.122993</td>\n",
       "      <td>0.360953</td>\n",
       "      <td>0.304467</td>\n",
       "      <td>0.243875</td>\n",
       "      <td>-0.259318</td>\n",
       "      <td>0.207682</td>\n",
       "      <td>0.606112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170762</td>\n",
       "      <td>-0.039057</td>\n",
       "      <td>-0.127535</td>\n",
       "      <td>-0.163997</td>\n",
       "      <td>0.143110</td>\n",
       "      <td>-0.122583</td>\n",
       "      <td>-0.143714</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.462609</td>\n",
       "      <td>0.190255</td>\n",
       "      <td>-0.135737</td>\n",
       "      <td>0.116034</td>\n",
       "      <td>0.353432</td>\n",
       "      <td>0.294504</td>\n",
       "      <td>0.248101</td>\n",
       "      <td>-0.259339</td>\n",
       "      <td>0.202299</td>\n",
       "      <td>0.578540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169872</td>\n",
       "      <td>-0.037982</td>\n",
       "      <td>-0.127574</td>\n",
       "      <td>-0.161002</td>\n",
       "      <td>0.141842</td>\n",
       "      <td>-0.117328</td>\n",
       "      <td>-0.141122</td>\n",
       "      <td>-0.012323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.459056</td>\n",
       "      <td>0.187457</td>\n",
       "      <td>-0.149122</td>\n",
       "      <td>0.108107</td>\n",
       "      <td>0.344656</td>\n",
       "      <td>0.284036</td>\n",
       "      <td>0.251216</td>\n",
       "      <td>-0.258564</td>\n",
       "      <td>0.197311</td>\n",
       "      <td>0.553323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168782</td>\n",
       "      <td>-0.036988</td>\n",
       "      <td>-0.127237</td>\n",
       "      <td>-0.158030</td>\n",
       "      <td>0.140913</td>\n",
       "      <td>-0.112311</td>\n",
       "      <td>-0.138505</td>\n",
       "      <td>-0.013541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   S1-D1_HbO  S1-D2_HbO  S2-D1_HbO  S2-D3_HbO  S3-D1_HbO  S3-D3_HbO   \n",
       "0   0.471891   0.202912  -0.093997   0.134281   0.372591   0.322716  \\\n",
       "1   0.469043   0.198057  -0.108167   0.129077   0.367336   0.313904   \n",
       "2   0.465940   0.193808  -0.122093   0.122993   0.360953   0.304467   \n",
       "3   0.462609   0.190255  -0.135737   0.116034   0.353432   0.294504   \n",
       "4   0.459056   0.187457  -0.149122   0.108107   0.344656   0.284036   \n",
       "\n",
       "   S3-D4_HbO  S4-D2_HbO  S4-D4_HbO  S4-D5_HbO  ...  S5-D6_HbR  S6-D4_HbR   \n",
       "0   0.232153  -0.256723   0.219447   0.666889  ...  -0.171817  -0.041364  \\\n",
       "1   0.238565  -0.258443   0.213415   0.635690  ...  -0.171419  -0.040187   \n",
       "2   0.243875  -0.259318   0.207682   0.606112  ...  -0.170762  -0.039057   \n",
       "3   0.248101  -0.259339   0.202299   0.578540  ...  -0.169872  -0.037982   \n",
       "4   0.251216  -0.258564   0.197311   0.553323  ...  -0.168782  -0.036988   \n",
       "\n",
       "   S6-D6_HbR  S6-D7_HbR  S7-D5_HbR  S7-D7_HbR  S8-D6_HbR  S8-D7_HbR   \n",
       "0  -0.126358  -0.169818   0.146536  -0.133517  -0.148632  -0.006896  \\\n",
       "1  -0.127127  -0.166950   0.144688  -0.128004  -0.146233  -0.008951   \n",
       "2  -0.127535  -0.163997   0.143110  -0.122583  -0.143714  -0.010777   \n",
       "3  -0.127574  -0.161002   0.141842  -0.117328  -0.141122  -0.012323   \n",
       "4  -0.127237  -0.158030   0.140913  -0.112311  -0.138505  -0.013541   \n",
       "\n",
       "   arousal_score  valence_score  \n",
       "0            0.0            0.0  \n",
       "1            0.0            0.0  \n",
       "2            0.0            0.0  \n",
       "3            0.0            0.0  \n",
       "4            0.0            0.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "train_data, test_data = train_test_split(data_1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data.drop(['arousal_score', 'valence_score', 'image_path'], axis=1))\n",
    "test_data_normalized = scaler.transform(test_data.drop(['arousal_score', 'valence_score', 'image_path'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP regression task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.02018042467534542\n",
      "Epoch 20/100, Loss: 0.040841978043317795\n",
      "Epoch 30/100, Loss: 0.0025714526418596506\n",
      "Epoch 40/100, Loss: 0.019988734275102615\n",
      "Epoch 50/100, Loss: 0.007794364355504513\n",
      "Epoch 60/100, Loss: 0.005270054098218679\n",
      "Epoch 70/100, Loss: 0.008183201774954796\n",
      "Epoch 80/100, Loss: 0.009495115838944912\n",
      "Epoch 90/100, Loss: 0.0024096586275845766\n",
      "Epoch 100/100, Loss: 0.01624155417084694\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_dataset = CustomDataset(train_data_normalized, train_data[['arousal_score', 'valence_score']].values)\n",
    "test_dataset = CustomDataset(test_data_normalized, test_data[['arousal_score', 'valence_score']].values)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "input_size = train_data_normalized.shape[1]\n",
    "output_size = 2  # Predicting arousal_score and valence_score\n",
    "hidden_size = 50\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.float()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE:  0.05128848232328892\n",
      "Test MSE:  0.01092806205386296\n",
      "Test RMSE:  0.10453737156568918\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_mae = 0\n",
    "    total_mse = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.float()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        mae = torch.abs(outputs - targets).mean().item()  # MAE\n",
    "        mse = ((outputs - targets) ** 2).mean().item()  # MSE\n",
    "\n",
    "        total_mae += mae * inputs.size(0)  # Multiply by batch size\n",
    "        total_mse += mse * inputs.size(0)  # Multiply by batch size\n",
    "        total_samples += inputs.size(0)\n",
    "\n",
    "    print('Test MAE: ', total_mae / total_samples)\n",
    "    print('Test MSE: ', total_mse / total_samples)\n",
    "    print('Test RMSE: ', (total_mse / total_samples) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted arousal: -1.04, Actual arousal: -1.00\n",
      "Predicted valence: 1.00, Actual valence: 1.00\n",
      "\n",
      "Predicted arousal: -0.02, Actual arousal: 0.00\n",
      "Predicted valence: -0.95, Actual valence: -1.00\n",
      "\n",
      "Predicted arousal: 0.03, Actual arousal: 0.00\n",
      "Predicted valence: -1.06, Actual valence: -1.00\n",
      "\n",
      "Predicted arousal: 0.01, Actual arousal: 0.00\n",
      "Predicted valence: -0.02, Actual valence: 0.00\n",
      "\n",
      "Predicted arousal: 1.01, Actual arousal: 1.00\n",
      "Predicted valence: 0.97, Actual valence: 1.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "model.eval()  # Switch to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.float()\n",
    "\n",
    "        # Make predictions\n",
    "        outputs = model(inputs)\n",
    "        arousal_predictions, valence_predictions = outputs[:, 0], outputs[:, 1]\n",
    "        \n",
    "        # Print the first few predictions\n",
    "        for arousal_prediction, valence_prediction, target_arousal, target_valence in zip(arousal_predictions[:5], valence_predictions[:5], targets[:, 0][:5], targets[:, 1][:5]):\n",
    "            print(f'Predicted arousal: {arousal_prediction:.2f}, Actual arousal: {target_arousal:.2f}')\n",
    "            print(f'Predicted valence: {valence_prediction:.2f}, Actual valence: {target_valence:.2f}\\n')\n",
    "\n",
    "        # Break after the first batch to only print a few predictions\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP multiclass classification problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the targets to be in the range [0, 4] instead of [-2, 2] because PyTorch's CrossEntropyLoss expects class labels to be in this format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, Loss: 2.9827427864074707\n",
      "Epoch 20/1000, Loss: 2.7489967346191406\n",
      "Epoch 30/1000, Loss: 2.6320855617523193\n",
      "Epoch 40/1000, Loss: 2.7617275714874268\n",
      "Epoch 50/1000, Loss: 2.4745981693267822\n",
      "Epoch 60/1000, Loss: 2.7816989421844482\n",
      "Epoch 70/1000, Loss: 2.669663667678833\n",
      "Epoch 80/1000, Loss: 2.932523250579834\n",
      "Epoch 90/1000, Loss: 2.806131362915039\n",
      "Epoch 100/1000, Loss: 2.6243016719818115\n",
      "Epoch 110/1000, Loss: 2.838048219680786\n",
      "Epoch 120/1000, Loss: 2.987449884414673\n",
      "Epoch 130/1000, Loss: 2.4545645713806152\n",
      "Epoch 140/1000, Loss: 2.7423462867736816\n",
      "Epoch 150/1000, Loss: 2.5729427337646484\n",
      "Epoch 160/1000, Loss: 2.4795849323272705\n",
      "Epoch 170/1000, Loss: 2.9867165088653564\n",
      "Epoch 180/1000, Loss: 2.676955461502075\n",
      "Epoch 190/1000, Loss: 2.951040267944336\n",
      "Epoch 200/1000, Loss: 2.5577430725097656\n",
      "Epoch 210/1000, Loss: 2.728386402130127\n",
      "Epoch 220/1000, Loss: 3.013456106185913\n",
      "Epoch 230/1000, Loss: 2.678410053253174\n",
      "Epoch 240/1000, Loss: 2.750190258026123\n",
      "Epoch 250/1000, Loss: 2.528984546661377\n",
      "Epoch 260/1000, Loss: 2.5941357612609863\n",
      "Epoch 270/1000, Loss: 2.7911758422851562\n",
      "Epoch 280/1000, Loss: 2.6750195026397705\n",
      "Epoch 290/1000, Loss: 2.749523878097534\n",
      "Epoch 300/1000, Loss: 2.9130687713623047\n",
      "Epoch 310/1000, Loss: 2.7892684936523438\n",
      "Epoch 320/1000, Loss: 2.625608205795288\n",
      "Epoch 330/1000, Loss: 2.9179837703704834\n",
      "Epoch 340/1000, Loss: 2.519319534301758\n",
      "Epoch 350/1000, Loss: 2.5012128353118896\n",
      "Epoch 360/1000, Loss: 2.659668207168579\n",
      "Epoch 370/1000, Loss: 2.6562063694000244\n",
      "Epoch 380/1000, Loss: 2.5388107299804688\n",
      "Epoch 390/1000, Loss: 3.000570058822632\n",
      "Epoch 400/1000, Loss: 2.836926221847534\n",
      "Epoch 410/1000, Loss: 2.565207004547119\n",
      "Epoch 420/1000, Loss: 2.588153600692749\n",
      "Epoch 430/1000, Loss: 2.598360776901245\n",
      "Epoch 440/1000, Loss: 2.9624831676483154\n",
      "Epoch 450/1000, Loss: 2.4148077964782715\n",
      "Epoch 460/1000, Loss: 2.8313941955566406\n",
      "Epoch 470/1000, Loss: 2.9338691234588623\n",
      "Epoch 480/1000, Loss: 2.8299500942230225\n",
      "Epoch 490/1000, Loss: 2.62139892578125\n",
      "Epoch 500/1000, Loss: 2.7819817066192627\n",
      "Epoch 510/1000, Loss: 2.853606700897217\n",
      "Epoch 520/1000, Loss: 2.802593946456909\n",
      "Epoch 530/1000, Loss: 3.07393479347229\n",
      "Epoch 540/1000, Loss: 2.6464436054229736\n",
      "Epoch 550/1000, Loss: 2.56584095954895\n",
      "Epoch 560/1000, Loss: 2.5189716815948486\n",
      "Epoch 570/1000, Loss: 2.6586971282958984\n",
      "Epoch 580/1000, Loss: 2.605618953704834\n",
      "Epoch 590/1000, Loss: 2.772573947906494\n",
      "Epoch 600/1000, Loss: 2.642522096633911\n",
      "Epoch 610/1000, Loss: 2.884326457977295\n",
      "Epoch 620/1000, Loss: 2.4478085041046143\n",
      "Epoch 630/1000, Loss: 2.7859997749328613\n",
      "Epoch 640/1000, Loss: 2.674089193344116\n",
      "Epoch 650/1000, Loss: 2.7628018856048584\n",
      "Epoch 660/1000, Loss: 2.6588242053985596\n",
      "Epoch 670/1000, Loss: 2.6648387908935547\n",
      "Epoch 680/1000, Loss: 2.9083988666534424\n",
      "Epoch 690/1000, Loss: 2.7633092403411865\n",
      "Epoch 700/1000, Loss: 2.3497402667999268\n",
      "Epoch 710/1000, Loss: 2.7609479427337646\n",
      "Epoch 720/1000, Loss: 2.6898880004882812\n",
      "Epoch 730/1000, Loss: 2.847383975982666\n",
      "Epoch 740/1000, Loss: 2.638029098510742\n",
      "Epoch 750/1000, Loss: 2.641702651977539\n",
      "Epoch 760/1000, Loss: 2.771759033203125\n",
      "Epoch 770/1000, Loss: 2.9583208560943604\n",
      "Epoch 780/1000, Loss: 2.5887014865875244\n",
      "Epoch 790/1000, Loss: 2.657222032546997\n",
      "Epoch 800/1000, Loss: 2.8292558193206787\n",
      "Epoch 810/1000, Loss: 2.504478693008423\n",
      "Epoch 820/1000, Loss: 2.681675434112549\n",
      "Epoch 830/1000, Loss: 2.8149783611297607\n",
      "Epoch 840/1000, Loss: 2.7398858070373535\n",
      "Epoch 850/1000, Loss: 2.7872233390808105\n",
      "Epoch 860/1000, Loss: 2.6316897869110107\n",
      "Epoch 870/1000, Loss: 2.585862874984741\n",
      "Epoch 880/1000, Loss: 2.816614866256714\n",
      "Epoch 890/1000, Loss: 2.7602672576904297\n",
      "Epoch 900/1000, Loss: 2.6310198307037354\n",
      "Epoch 910/1000, Loss: 2.875715494155884\n",
      "Epoch 920/1000, Loss: 2.6115593910217285\n",
      "Epoch 930/1000, Loss: 2.5860390663146973\n",
      "Epoch 940/1000, Loss: 2.815802812576294\n",
      "Epoch 950/1000, Loss: 2.6472630500793457\n",
      "Epoch 960/1000, Loss: 2.89727520942688\n",
      "Epoch 970/1000, Loss: 2.766695499420166\n",
      "Epoch 980/1000, Loss: 2.7149319648742676\n",
      "Epoch 990/1000, Loss: 2.954676389694214\n",
      "Epoch 1000/1000, Loss: 2.731781244277954\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_dataset = CustomDataset(train_data_normalized, train_data[['arousal_score', 'valence_score']].values)\n",
    "test_dataset = CustomDataset(test_data_normalized, test_data[['arousal_score', 'valence_score']].values)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "input_size = train_data_normalized.shape[1]\n",
    "output_size = 2  # Predicting arousal_score and valence_score\n",
    "hidden_size = 200\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net(input_size, hidden_size, 2)  # Output size is 2\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# Update your targets to be within [0, 4]\n",
    "train_dataset.targets = train_dataset.targets + 2\n",
    "test_dataset.targets = test_dataset.targets + 2\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.float()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data for arousal_score: 9 %\n",
      "Accuracy of the network on the test data for valence_score: 10 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct_arousal = 0\n",
    "correct_valence = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.float()\n",
    "        targets = targets.long()  # CrossEntropyLoss expects targets to be of type long\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        \n",
    "        predicted_arousal = predicted[0]\n",
    "        predicted_valence = predicted[1]\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct_arousal += (predicted_arousal == targets[:, 0]).sum().item()\n",
    "        correct_valence += (predicted_valence == targets[:, 1]).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test data for arousal_score: %d %%' % (\n",
    "    100 * correct_arousal / total))\n",
    "\n",
    "print('Accuracy of the network on the test data for valence_score: %d %%' % (\n",
    "    100 * correct_valence / total))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000, Loss: 1.6886212825775146\n",
      "Epoch 20/1000, Loss: 1.2943766117095947\n",
      "Epoch 30/1000, Loss: 1.550680160522461\n",
      "Epoch 40/1000, Loss: 1.1466724872589111\n",
      "Epoch 50/1000, Loss: 1.1815441846847534\n",
      "Epoch 60/1000, Loss: 1.2847070693969727\n",
      "Epoch 70/1000, Loss: 1.1381614208221436\n",
      "Epoch 80/1000, Loss: 0.827776312828064\n",
      "Epoch 90/1000, Loss: 1.1725575923919678\n",
      "Epoch 100/1000, Loss: 1.1144821643829346\n",
      "Epoch 110/1000, Loss: 0.9603128433227539\n",
      "Epoch 120/1000, Loss: 1.1071457862854004\n",
      "Epoch 130/1000, Loss: 1.2013905048370361\n",
      "Epoch 140/1000, Loss: 1.1059744358062744\n",
      "Epoch 150/1000, Loss: 0.9586115479469299\n",
      "Epoch 160/1000, Loss: 1.0871164798736572\n",
      "Epoch 170/1000, Loss: 1.203660249710083\n",
      "Epoch 180/1000, Loss: 0.9663504362106323\n",
      "Epoch 190/1000, Loss: 1.0540177822113037\n",
      "Epoch 200/1000, Loss: 1.1633906364440918\n",
      "Epoch 210/1000, Loss: 1.1072871685028076\n",
      "Epoch 220/1000, Loss: 1.0720463991165161\n",
      "Epoch 230/1000, Loss: 1.0737779140472412\n",
      "Epoch 240/1000, Loss: 1.0027353763580322\n",
      "Epoch 250/1000, Loss: 1.1597020626068115\n",
      "Epoch 260/1000, Loss: 1.0636589527130127\n",
      "Epoch 270/1000, Loss: 0.9630500078201294\n",
      "Epoch 280/1000, Loss: 0.9497159719467163\n",
      "Epoch 290/1000, Loss: 1.0962579250335693\n",
      "Epoch 300/1000, Loss: 1.1528687477111816\n",
      "Epoch 310/1000, Loss: 1.0602648258209229\n",
      "Epoch 320/1000, Loss: 1.0920863151550293\n",
      "Epoch 330/1000, Loss: 1.112487554550171\n",
      "Epoch 340/1000, Loss: 1.109825849533081\n",
      "Epoch 350/1000, Loss: 0.9667345285415649\n",
      "Epoch 360/1000, Loss: 0.9082446694374084\n",
      "Epoch 370/1000, Loss: 0.902572751045227\n",
      "Epoch 380/1000, Loss: 1.1080772876739502\n",
      "Epoch 390/1000, Loss: 1.101216197013855\n",
      "Epoch 400/1000, Loss: 0.9614561796188354\n",
      "Epoch 410/1000, Loss: 1.080674409866333\n",
      "Epoch 420/1000, Loss: 1.043318271636963\n",
      "Epoch 430/1000, Loss: 1.1536929607391357\n",
      "Epoch 440/1000, Loss: 0.9357284307479858\n",
      "Epoch 450/1000, Loss: 0.8963561058044434\n",
      "Epoch 460/1000, Loss: 0.9894462823867798\n",
      "Epoch 470/1000, Loss: 1.1394258737564087\n",
      "Epoch 480/1000, Loss: 1.0160953998565674\n",
      "Epoch 490/1000, Loss: 0.9605329036712646\n",
      "Epoch 500/1000, Loss: 0.9458374977111816\n",
      "Epoch 510/1000, Loss: 1.0242335796356201\n",
      "Epoch 520/1000, Loss: 0.8568989038467407\n",
      "Epoch 530/1000, Loss: 1.1981894969940186\n",
      "Epoch 540/1000, Loss: 1.0928752422332764\n",
      "Epoch 550/1000, Loss: 1.099545955657959\n",
      "Epoch 560/1000, Loss: 1.0450818538665771\n",
      "Epoch 570/1000, Loss: 0.9560677409172058\n",
      "Epoch 580/1000, Loss: 1.012169361114502\n",
      "Epoch 590/1000, Loss: 1.143306851387024\n",
      "Epoch 600/1000, Loss: 0.9923044443130493\n",
      "Epoch 610/1000, Loss: 0.949803352355957\n",
      "Epoch 620/1000, Loss: 1.030163049697876\n",
      "Epoch 630/1000, Loss: 0.9456652402877808\n",
      "Epoch 640/1000, Loss: 0.6977472305297852\n",
      "Epoch 650/1000, Loss: 1.1139683723449707\n",
      "Epoch 660/1000, Loss: 1.1563010215759277\n",
      "Epoch 670/1000, Loss: 1.1569395065307617\n",
      "Epoch 680/1000, Loss: 1.0636234283447266\n",
      "Epoch 690/1000, Loss: 1.2059760093688965\n",
      "Epoch 700/1000, Loss: 1.0026075839996338\n",
      "Epoch 710/1000, Loss: 1.0913522243499756\n",
      "Epoch 720/1000, Loss: 1.044290542602539\n",
      "Epoch 730/1000, Loss: 1.092703104019165\n",
      "Epoch 740/1000, Loss: 0.8618118762969971\n",
      "Epoch 750/1000, Loss: 0.8547266721725464\n",
      "Epoch 760/1000, Loss: 1.019823431968689\n",
      "Epoch 770/1000, Loss: 0.9923946857452393\n",
      "Epoch 780/1000, Loss: 1.0088481903076172\n",
      "Epoch 790/1000, Loss: 1.1576075553894043\n",
      "Epoch 800/1000, Loss: 1.1994330883026123\n",
      "Epoch 810/1000, Loss: 0.9554003477096558\n",
      "Epoch 820/1000, Loss: 1.044247031211853\n",
      "Epoch 830/1000, Loss: 1.0422778129577637\n",
      "Epoch 840/1000, Loss: 1.1073192358016968\n",
      "Epoch 850/1000, Loss: 1.0921359062194824\n",
      "Epoch 860/1000, Loss: 0.84554123878479\n",
      "Epoch 870/1000, Loss: 0.9923251271247864\n",
      "Epoch 880/1000, Loss: 1.2464706897735596\n",
      "Epoch 890/1000, Loss: 1.1908419132232666\n",
      "Epoch 900/1000, Loss: 1.17567777633667\n",
      "Epoch 910/1000, Loss: 0.9416648149490356\n",
      "Epoch 920/1000, Loss: 0.9064196348190308\n",
      "Epoch 930/1000, Loss: 1.1687088012695312\n",
      "Epoch 940/1000, Loss: 1.001952886581421\n",
      "Epoch 950/1000, Loss: 1.0422580242156982\n",
      "Epoch 960/1000, Loss: 1.1920075416564941\n",
      "Epoch 970/1000, Loss: 1.1447246074676514\n",
      "Epoch 980/1000, Loss: 0.9582618474960327\n",
      "Epoch 990/1000, Loss: 0.8934630155563354\n",
      "Epoch 1000/1000, Loss: 1.1415786743164062\n",
      "Accuracy of the network on the test data for arousal_score: 63.75 %\n",
      "Accuracy of the network on the test data for valence_score: 60.5 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess data\n",
    "features = data_1.iloc[:, :-2].values\n",
    "arousal_score = LabelEncoder().fit_transform(data_1.iloc[:, -2] + 2)  # Mapping -2 -> 0, -1 -> 1, 0 -> 2, 1 -> 3, 2 -> 4\n",
    "valence_score = LabelEncoder().fit_transform(data_1.iloc[:, -1] + 2)  # Same mapping for valence_score\n",
    "targets = list(zip(arousal_score, valence_score))\n",
    "\n",
    "# Split data\n",
    "train_features, test_features, train_targets, test_targets = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = features.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = 5  # Classes representing -2, -1, 0, 1, 2\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create DataLoaders\n",
    "train_data = TensorDataset(torch.tensor(train_features).float(), torch.tensor(train_targets).long())\n",
    "test_data = TensorDataset(torch.tensor(test_features).float(), torch.tensor(test_targets).long())\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss and optimizer\n",
    "model = LSTM(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1, 1, input_size)\n",
    "        targets_arousal = targets[:, 0]\n",
    "        targets_valence = targets[:, 1]\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss_arousal = criterion(outputs, targets_arousal)\n",
    "        loss_valence = criterion(outputs, targets_valence)\n",
    "\n",
    "        loss = loss_arousal + loss_valence  # Total loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Testing\n",
    "correct_arousal, correct_valence = 0, 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.view(-1, 1, input_size)\n",
    "        targets_arousal = targets[:, 0]\n",
    "        targets_valence = targets[:, 1]\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _, predicted_arousal = torch.max(outputs.data, 1)\n",
    "        _, predicted_valence = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct_arousal += (predicted_arousal == targets_arousal).sum().item()\n",
    "        correct_valence += (predicted_valence == targets_valence).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test data for arousal_score: {100 * correct_arousal / total} %')\n",
    "print(f'Accuracy of the network on the test data for valence_score: {100 * correct_valence / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
